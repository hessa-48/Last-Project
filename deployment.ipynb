!pip install transformers
pip install openai
pip install nltk
pip install scikit-learn
pip install scikit-learn
pip install gradio
!pip install pyarrow
pip install sentence-transformers
pip install protobuf
!pip install torch --pre --index-url https://download.pytorch.org/whl/nightly/cpu
!pip install torch

  from transformers import AutoTokenizer

model_path = "C:/Users/Huawei/Desktop/classification model"
tokenizer = AutoTokenizer.from_pretrained(model_path)


  from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_path = "C:/Users/Huawei/Desktop/classification model"  # Update this path if necessary

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Print the model and tokenizer to verify everything is loaded
print(model)
print(tokenizer)


    RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-5): 6 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=3, bias=True)
  )
)
RobertaTokenizerFast(name_or_path='C:/Users/Huawei/Desktop/classification model', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),
}
)





    from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_path = "C:/Users/Huawei/Desktop/classification model"

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Move model to device (CPU or CUDA if available)
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print("Model and tokenizer loaded successfully üéâ")




    import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import joblib

# Load the embeddings (assuming it's a NumPy array)
embeddings = np.load(r"C:\Users\Huawei\Desktop\clustring model\embeddings.npy")

# Load the pre-trained KMeans model
kmeans_model = joblib.load(r"C:\Users\Huawei\Desktop\clustring model\kmeans_model.pkl")

# Load the reviews if you plan to use them
reviews = pd.read_parquet(r"C:\Users\Huawei\Desktop\clustring model\reviews_processed.parquet")







                       # the final projectt
import gradio as gr
import pandas as pd
from collections import defaultdict
import nltk
from openai import OpenAI
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import joblib
from sentence_transformers import SentenceTransformer

# Setup
nltk.download('punkt')
nltk.download('stopwords')

device = 0 if torch.cuda.is_available() else -1

# GPT-4 setup (keep your actual key here)
client = OpenAI(api_key="my key")

# Classification pipeline
model_path = "C:/Users/Huawei/Desktop/classification model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=device)

# Category clusters
cluster_mapping = {
    0: "Online offers",
    1: "electronics",
    2: "For Kids",
    3: "E-Readers & Home",
    4: "best purchase",
    5: "Voice-Enabled Tab"
}

# Summarization helper functions
def convert_df_to_json(df, selected_category):
    grouped_data = defaultdict(lambda: {"reviews": [], "category": ""})
    for _, row in df.iterrows():
        if str(row.get('category')) != selected_category:
            continue
        product = row.get('name')
        review_text = row.get('reviews.text')
        rating = row.get('reviews.rating')
        if pd.notnull(review_text) and pd.notnull(rating):
            grouped_data[product]["reviews"].append({
                "text": str(review_text),
                "rating": float(rating)
            })
            grouped_data[product]["category"] = selected_category

    structured_reviews = []
    for product, details in grouped_data.items():
        if len(details["reviews"]) >= 2:
            ratings = [r["rating"] for r in details["reviews"]]
            texts = [r["text"] for r in details["reviews"]]
            structured_reviews.append({
                "product_name": product,
                "category": details["category"],
                "avg_rating": sum(ratings) / len(ratings),
                "top_pros": [t for t in texts if "good" in t.lower() or "great" in t.lower()][:2],
                "top_complaints": [t for t in texts if "bad" in t.lower() or "disappoint" in t.lower()][:2]
            })
    return structured_reviews

def build_insight_string(products):
    sorted_products = sorted(products, key=lambda x: x["avg_rating"], reverse=True)
    top_3 = sorted_products[:3]
    worst = sorted_products[-1]

    insights_str = ""
    for idx, p in enumerate(top_3, 1):
        insights_str += f"""{idx}. {p['product_name']} - Rating: {p['avg_rating']:.2f}
Key Pros: {", ".join(p['top_pros']) or "N/A"}
Top Complaints: {", ".join(p['top_complaints']) or "N/A"}\n\n"""

    insights_str += f"Worst Product:\n{worst['product_name']} - Rating: {worst['avg_rating']:.2f}\n"
    insights_str += f"Complaints: {', '.join(worst['top_complaints']) or 'N/A'}"
    return insights_str

def generate_article(category, insights):
    prompt = f"""
You are a professional tech writer.

Write a clear, structured summary about the product category: "{category}".

The summary should include:
1. Top 3 products in this category and their key differences.
2. Top complaints for each of those top 3 products.
3. The worst product and why users should avoid it.

Here are the insights to use:
{insights}
"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=900
    )
    return response.choices[0].message.content

# Classification logic
def classify_text(text):
    result = classifier(text)
    label = result[0]['label']
    return {"LABEL_0": "Negative", "LABEL_1": "Neutral", "LABEL_2": "Positive"}.get(label, "Unknown")

# Summary generation logic
def handle_summary(file, category):
    if not file:
        return "Please upload a file."
    if not category:
        return "Please select a category."
    try:
        df = pd.read_csv(file.name)
        df.rename(columns={"categories": "category"}, inplace=True)  # ‚úÖ FIXED COLUMN NAME
        insights = convert_df_to_json(df, category)
        if not insights:
            return f"No sufficient reviews for the selected category: {category}"
        insight_str = build_insight_string(insights)
        return generate_article(category, insight_str)
    except Exception as e:
        return f"Error: {e}"

# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ü§ñ ÿßŸÑÿ≠ÿßŸÉŸÖ")

    with gr.Tab("üîç Classification"):
        txt = gr.Textbox(label="Enter text to classify")
        out = gr.Label()
        classify_btn = gr.Button("Classify")
        classify_btn.click(classify_text, txt, out)

    with gr.Tab("üìö Category Summary"):
        gr.Markdown("### Step 1: Select a Category")
        selected_category = gr.Radio(choices=list(cluster_mapping.values()), label="Select Category")

        gr.Markdown("### Step 2: Upload Your CSV")
        file_input = gr.File(label="Upload CSV", file_types=[".csv"])
        generate_btn = gr.Button("Generate Summary")
        summary_output = gr.Textbox(label="Generated Summary", lines=25)

        generate_btn.click(fn=handle_summary, inputs=[file_input, selected_category], outputs=summary_output)

demo.launch()

